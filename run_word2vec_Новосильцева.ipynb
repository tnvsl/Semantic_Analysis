{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9rUEK67O-UN"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"5-XkBeYFO-US"},"source":["\n","Word2Vec Model\n","==============\n","\n","Introduces Gensim's Word2Vec model and demonstrates its use on the `Lee Evaluation Corpus\n","<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5uUnNDFO-UU"},"outputs":[],"source":["import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"]},{"cell_type":"markdown","metadata":{"id":"-DH8N6l7O-UV"},"source":["In case you missed the buzz, Word2Vec is a widely used algorithm based on neural\n","networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n","Using large amounts of unannotated plain text, word2vec learns relationships\n","between words automatically. The output are vectors, one vector per word,\n","with remarkable linear relationships that allow us to do things like:\n","\n","* vec(\"king\") - vec(\"man\") + vec(\"woman\") =~ vec(\"queen\")\n","* vec(\"Montreal Canadiens\") – vec(\"Montreal\") + vec(\"Toronto\") =~ vec(\"Toronto Maple Leafs\").\n","\n","Word2vec is very useful in `automatic text tagging\n","<https://github.com/RaRe-Technologies/movie-plots-by-genre>`_\\ , recommender\n","systems and machine translation.\n","\n","This tutorial:\n","\n","#. Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n","#. Shows off a demo of ``Word2Vec`` using a pre-trained model\n","#. Demonstrates training a new model from your own data\n","#. Demonstrates loading and saving models\n","#. Introduces several training parameters and demonstrates their effect\n","#. Discusses memory requirements\n","#. Visualizes Word2Vec embeddings by applying dimensionality reduction\n","\n","Review: Bag-of-words\n","--------------------\n","\n",".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n","\n","You may be familiar with the `bag-of-words model\n","<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n","`core_concepts_vector` section.\n","This model transforms each document to a fixed-length vector of integers.\n","For example, given the sentences:\n","\n","- ``John likes to watch movies. Mary likes movies too.``\n","- ``John also likes to watch football games. Mary hates football.``\n","\n","The model outputs the vectors:\n","\n","- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n","- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n","\n","Each vector has 10 elements, where each element counts the number of times a\n","particular word occurred in the document.\n","The order of elements is arbitrary.\n","In the example above, the order of the elements corresponds to the words:\n","``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n","\n","Bag-of-words models are surprisingly effective, but have several weaknesses.\n","\n","First, they lose all information about word order: \"John likes Mary\" and\n","\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n","of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n","models consider word phrases of length n to represent documents as\n","fixed-length vectors to capture local word order but suffer from data\n","sparsity and high dimensionality.\n","\n","Second, the model does not attempt to learn the meaning of the underlying\n","words, and as a consequence, the distance between vectors doesn't always\n","reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n","second problem.\n","\n","Introducing: the ``Word2Vec`` Model\n","-----------------------------------\n","\n","``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n","vector space using a shallow neural network. The result is a set of\n","word-vectors where vectors close together in vector space have similar\n","meanings based on context, and word-vectors distant to each other have\n","differing meanings. For example, ``strong`` and ``powerful`` would be close\n","together and ``strong`` and ``Paris`` would be relatively far.\n","\n","The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\n","class implements them both:\n","\n","1. Skip-grams (SG)\n","2. Continuous-bag-of-words (CBOW)\n","\n",".. Important::\n","  Don't let the implementation details below scare you.\n","  They're advanced material: if it's too much, then move on to the next section.\n","\n","The `Word2Vec Skip-gram <http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model>`__\n","model, for example, takes in pairs (word1, word2) generated by moving a\n","window across text data, and trains a 1-hidden-layer neural network based on\n","the synthetic task of given an input word, giving us a predicted probability\n","distribution of nearby words to the input. A virtual `one-hot\n","<https://en.wikipedia.org/wiki/One-hot>`__ encoding of words\n","goes through a 'projection layer' to the hidden layer; these projection\n","weights are later interpreted as the word embeddings. So if the hidden layer\n","has 300 neurons, this network will give us 300-dimensional word embeddings.\n","\n","Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It\n","is also a 1-hidden-layer neural network. The synthetic training task now uses\n","the average of multiple input context words, rather than a single word as in\n","skip-gram, to predict the center word. Again, the projection weights that\n","turn one-hot words into averageable vectors, of the same width as the hidden\n","layer, are interpreted as the word embeddings.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3BUu0-MoO-UY"},"source":["Word2Vec Demo\n","-------------\n","\n","To see what ``Word2Vec`` can do, let's download a pre-trained model and play\n","around with it. We will fetch the Word2Vec model trained on part of the\n","Google News dataset, covering approximately 3 million words and phrases. Such\n","a model can take hours to train, but since it's already available,\n","downloading and loading it with Gensim takes minutes.\n","\n",".. Important::\n","  The model is approximately 2GB, so you'll need a decent network connection\n","  to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n","  below.\n","\n","You may also check out an `online word2vec demo\n","<http://radimrehurek.com/2014/02/word2vec-tutorial/#app>`_ where you can try\n","this vector algebra for yourself. That demo runs ``word2vec`` on the\n","**entire** Google News dataset, of **about 100 billion words**.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14802,"status":"ok","timestamp":1666648051149,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"WN-OBJKqbWfI","outputId":"fbebb3db-7c82-4835-d44d-badd0c849c95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Collecting gensim\n","  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n","\u001b[K     |████████████████████████████████| 24.1 MB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-4.2.0\n"]}],"source":["!pip install --upgrade gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":761898,"status":"ok","timestamp":1666649308916,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"y_22ZdQFO-UZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf4136e5-8427-48f0-d7a0-8709e501cdf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[=================================================-] 99.8% 1659.2/1662.8MB downloaded\n"]}],"source":["import gensim.downloader as api\n","wv = api.load('word2vec-google-news-300')"]},{"cell_type":"markdown","metadata":{"id":"rpodV_hIO-Ua"},"source":["A common operation is to retrieve the vocabulary of a model. That is trivial:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268,"status":"ok","timestamp":1666649312732,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"ycmKvTHDO-Ub","outputId":"0380c563-4a9b-4344-ec90-60e3d3941380"},"outputs":[{"output_type":"stream","name":"stdout","text":["word #0/3000000 is </s>\n","word #1/3000000 is in\n","word #2/3000000 is for\n","word #3/3000000 is that\n","word #4/3000000 is is\n","word #5/3000000 is on\n","word #6/3000000 is ##\n","word #7/3000000 is The\n","word #8/3000000 is with\n","word #9/3000000 is said\n"]}],"source":["for index, word in enumerate(wv.index_to_key):\n","    if index == 10:\n","        break\n","    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"]},{"cell_type":"markdown","metadata":{"id":"XgRo6P72O-Uc"},"source":["We can easily obtain vectors for terms the model is familiar with:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":250,"status":"ok","timestamp":1666649314520,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"NtFiB5iUO-Ue","outputId":"fd7370d3-8e1e-48cf-c2bc-554f331bd168"},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n"," -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n","  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n"," -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n","  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n","  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n","  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n","  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n","  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n","  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n","  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n","  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n"," -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n"," -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n"," -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n","  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n","  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n"," -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n"," -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n"," -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n","  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n","  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n","  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n"," -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n"," -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n","  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n"," -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n","  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n"," -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n"," -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n","  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n"," -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n"," -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n","  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n"," -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n","  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n","  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n"," -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n"," -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n"," -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n"," -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n"," -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n","  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n","  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n","  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n"," -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n","  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n","  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n"," -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n"," -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n"," -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n","  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n","  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n"," -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n","  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n"," -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n","  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n","  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n"," -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n","  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n","  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n","  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n","  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n","  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n"," -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n","  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n"," -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n","  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n","  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n"," -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n","  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n","  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n","  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n"," -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n"," -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"]}],"source":["vec_king = wv['king']\n","print(vec_king)"]},{"cell_type":"markdown","metadata":{"id":"yK2U4d9MO-Ug"},"source":["Unfortunately, the model is unable to infer vectors for unfamiliar words.\n","This is one limitation of Word2Vec: if this limitation matters to you, check\n","out the FastText model.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251,"status":"ok","timestamp":1666649322960,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"0cQ2AM9YO-Uh","outputId":"2976fbf6-9b5d-4ab0-d5e7-95d9966bd616"},"outputs":[{"output_type":"stream","name":"stdout","text":["The word 'cameroon' does not appear in this model\n"]}],"source":["try:\n","    vec_cameroon = wv['cameroon']\n","except KeyError:\n","    print(\"The word 'cameroon' does not appear in this model\")"]},{"cell_type":"markdown","metadata":{"id":"Cl2_8-oXO-Uh"},"source":["Moving on, ``Word2Vec`` supports several word similarity tasks out of the\n","box.  You can see how the similarity intuitively decreases as the words get\n","less and less similar.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1666649327258,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"MCklNxaVO-Ui","outputId":"767a26cd-3969-44c6-da02-8ab608aa934c"},"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t0.69\n","'car'\t'bicycle'\t0.54\n","'car'\t'airplane'\t0.42\n","'car'\t'cereal'\t0.14\n","'car'\t'communism'\t0.06\n"]}],"source":["pairs = [\n","    ('car', 'minivan'),   # a minivan is a kind of car\n","    ('car', 'bicycle'),   # still a wheeled vehicle\n","    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n","    ('car', 'cereal'),    # ... and so on\n","    ('car', 'communism'),\n","]\n","for w1, w2 in pairs:\n","    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"]},{"cell_type":"markdown","source":["**DISTANCES**"],"metadata":{"id":"hV_mRcIT9K68"}},{"cell_type":"code","source":["from scipy.spatial import distance\n","for w1, w2 in pairs:\n","  print('%r\\t%r\\t%.2f' % (w1, w2, distance.cosine(wv[w1], wv[w2])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJZbaSOJrI8G","executionInfo":{"status":"ok","timestamp":1666649329877,"user_tz":240,"elapsed":380,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"fd2cd5cb-213e-4745-b75b-ba48455f7a4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t0.31\n","'car'\t'bicycle'\t0.46\n","'car'\t'airplane'\t0.58\n","'car'\t'cereal'\t0.86\n","'car'\t'communism'\t0.94\n"]}]},{"cell_type":"code","source":["for w1, w2 in pairs:\n","  print('%r\\t%r\\t%.2f' % (w1, w2, distance.cityblock(wv[w1], wv[w2])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJsK1qGurXlD","executionInfo":{"status":"ok","timestamp":1666649331578,"user_tz":240,"elapsed":290,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"9b1be90d-36b7-44e7-dd73-8c1ac3d70a2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t34.63\n","'car'\t'bicycle'\t37.79\n","'car'\t'airplane'\t43.13\n","'car'\t'cereal'\t54.26\n","'car'\t'communism'\t63.66\n"]}]},{"cell_type":"code","source":["for w1, w2 in pairs:\n","  print('%r\\t%r\\t%.2f' % (w1, w2, distance.euclidean(wv[w1], wv[w2])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4p61nlbrbj3","executionInfo":{"status":"ok","timestamp":1666649333263,"user_tz":240,"elapsed":271,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"bcc27686-9486-479e-a826-097da0e0a52e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t2.53\n","'car'\t'bicycle'\t2.70\n","'car'\t'airplane'\t3.08\n","'car'\t'cereal'\t3.99\n","'car'\t'communism'\t4.58\n"]}]},{"cell_type":"code","source":["for w1, w2 in pairs:\n","  print('%r\\t%r\\t%.2f' % (w1, w2, distance.minkowski(wv[w1], wv[w2])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sI3TToGrgCg","executionInfo":{"status":"ok","timestamp":1666649334920,"user_tz":240,"elapsed":3,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"7cda5510-6db1-4a69-8149-2271120da6f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t2.53\n","'car'\t'bicycle'\t2.70\n","'car'\t'airplane'\t3.08\n","'car'\t'cereal'\t3.99\n","'car'\t'communism'\t4.58\n"]}]},{"cell_type":"code","source":["for w1, w2 in pairs:\n","  print('%r\\t%r\\t%.2f' % (w1, w2, distance.sqeuclidean(wv[w1], wv[w2])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeSav-7ErmwL","executionInfo":{"status":"ok","timestamp":1666649336661,"user_tz":240,"elapsed":260,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"15325951-70f2-4589-d23f-e8e31e8f70ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'car'\t'minivan'\t6.41\n","'car'\t'bicycle'\t7.32\n","'car'\t'airplane'\t9.51\n","'car'\t'cereal'\t15.94\n","'car'\t'communism'\t20.99\n"]}]},{"cell_type":"markdown","metadata":{"id":"K0F5XtuDO-Ui"},"source":["Print the 5 most similar words to \"car\" or \"minivan\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5362,"status":"ok","timestamp":1666649348034,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"-pFkBn-aO-Uj","outputId":"37c8a120-ecba-4716-8a93-d3b4db9d2375"},"outputs":[{"output_type":"stream","name":"stdout","text":["[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763688564300537), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565720081329346)]\n"]}],"source":["print(wv.most_similar(positive=['car', 'minivan'], topn=5))"]},{"cell_type":"markdown","metadata":{"id":"M_CftdMSO-Uj"},"source":["Which of the below does not belong in the sequence?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1666649352273,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"},"user_tz":240},"id":"GvmnLBkDO-Uk","outputId":"69d6998f-8905-4216-8443-0efbc4cc89f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["car\n"]}],"source":["print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"]},{"cell_type":"markdown","metadata":{"id":"2XCaaEpcO-Uk"},"source":["Training Your Own Model\n","-----------------------\n","\n","To start, you'll need some data for training the model. For the following\n","examples, we'll use the `Lee Evaluation Corpus\n","<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n","(which you `already have\n","<https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor>`_\n","if you've installed Gensim).\n","\n","This corpus is small enough to fit entirely in memory, but we'll implement a\n","memory-friendly iterator that reads it line-by-line to demonstrate how you\n","would handle a larger corpus.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pnbvZiVO-Uk"},"outputs":[],"source":["from gensim.test.utils import datapath\n","from gensim import utils\n","\n","class MyCorpus:\n","    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n","\n","    def __iter__(self):\n","        corpus_path = datapath('lee_background.cor')\n","        for line in open(corpus_path):\n","            # assume there's one document per line, tokens separated by whitespace\n","            yield utils.simple_preprocess(line)"]},{"cell_type":"markdown","metadata":{"id":"f7yB_0QPO-Ul"},"source":["If we wanted to do any custom preprocessing, e.g. decode a non-standard\n","encoding, lowercase, remove numbers, extract named entities... All of this can\n","be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to\n","know. All that is required is that the input yields one sentence (list of\n","utf8 words) after another.\n","\n","Let's go ahead and train a model on our corpus.  Don't worry about the\n","training parameters much for now, we'll revisit them later.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6s7OVNbO-Um"},"outputs":[],"source":["import gensim.models\n","\n","sentences = MyCorpus()\n","model = gensim.models.Word2Vec(sentences=sentences)"]},{"cell_type":"markdown","metadata":{"id":"_GaF7duJO-Um"},"source":["Once we have our model, we can use it in the same way as in the demo above.\n","\n","The main part of the model is ``model.wv``\\ , where \"wv\" stands for \"word vectors\".\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vX_IW8bO-Um"},"outputs":[],"source":["#@title Текст заголовка по умолчанию\n","# vec_king = model.wv['king']\n","# print(vec_king)"]},{"cell_type":"markdown","source":["**WINDOW SIZE**"],"metadata":{"id":"P83qHoiU-t_6"}},{"cell_type":"code","source":["model3 = gensim.models.Word2Vec(sentences=sentences, window=3)\n","model5 = gensim.models.Word2Vec(sentences=sentences, window=5)\n","model8 = gensim.models.Word2Vec(sentences=sentences, window=8)"],"metadata":{"id":"rukIfYG3_WFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model3.wv.most_similar('car', topn=10))\n","print(model5.wv.most_similar('car', topn=10))\n","print(model8.wv.most_similar('car', topn=10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDvm1LQq-_gx","executionInfo":{"status":"ok","timestamp":1666649428440,"user_tz":240,"elapsed":242,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"575801ad-88df-4a9f-b7a8-79db78a4d6b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('four', 0.9973411560058594), ('week', 0.9972687363624573), ('world', 0.997134268283844), ('out', 0.9971265196800232), ('just', 0.9971011877059937), ('east', 0.9970937371253967), ('made', 0.9970842003822327), ('by', 0.9970813393592834), ('reports', 0.9970569014549255), ('metres', 0.9970553517341614)]\n","[('four', 0.996623694896698), ('week', 0.9965763092041016), ('more', 0.9964373111724854), ('made', 0.9964262247085571), ('out', 0.9964209198951721), ('adelaide', 0.996415913105011), ('reports', 0.9964086413383484), ('east', 0.996390163898468), ('metres', 0.9963768720626831), ('place', 0.9963710308074951)]\n","[('four', 0.9963157176971436), ('week', 0.9963067173957825), ('reports', 0.9961479902267456), ('world', 0.9961386919021606), ('made', 0.9961257576942444), ('adelaide', 0.9961252808570862), ('metres', 0.9960952997207642), ('more', 0.9960907697677612), ('out', 0.9960852861404419), ('place', 0.9960778951644897)]\n"]}]},{"cell_type":"markdown","source":["*PREDICTIONS FOR PARTS OF SPEECH *"],"metadata":{"id":"aUfNpjv9EB31"}},{"cell_type":"code","source":["import nltk\n","wordtags = nltk.ConditionalFreqDist((w.lower(), t) \n","for w, t in nltk.corpus.brown.tagged_words(tagset=\"universal\"))"],"metadata":{"id":"I7MGmpuSEdVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predictions_for_adj(word):\n","\n","  tuple_sim = model.wv.most_similar([word],  topn=100)\n","  predictions = []\n","  print('Predictions for', word)\n","  for i in tuple_sim:\n","    try:\n","      dict_word_tag = dict(wordtags[i[0]])\n","      tag = list(dict_word_tag.keys())[0]\n","      if tag == \"NOUN\":\n","        predictions.append(i[0])\n","\n","    except IndexError:\n","      pass\n","  print(predictions)"],"metadata":{"id":"18KGOrT_EATh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_for_adj('nice')\n","predictions_for_adj('clean')\n","predictions_for_adj('blue')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmTs_pr7E5j8","executionInfo":{"status":"ok","timestamp":1666650702565,"user_tz":240,"elapsed":8,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"da36b931-c9de-48d7-ee89-425286b33e90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions for nice\n","['britain', 'guides', 'people', 'crisis', 'trade', 'arrest', 'hicks', 'anything', 'cup', 'test', 'relations', 'plane', 'area', 'david', 'hospital', 'director', 'cancer', 'africa', 'brisbane', 're', 'children', 'power', 'year', 'days', 'staff', 'station', 'parties', 'home', 'captain', 'officials', 'india', 'river', 'part', 'yesterday', 'statement']\n","Predictions for clean\n","['july', 'wickets', 'alliance', 'safety', 'reports', 'days', 'strikes', 'afghan', 'lee', 'africa', 'yesterday', 'water', 'hill', 'deaths', 'country', 'minister', 'george', 'state', 'test', 'disease', 'authorities', 'company', 'time', 'economy', 'secretary', 'race', 'commission', 'president', 'decision', 'line', 'workers', 'statement', 'hospital']\n","Predictions for blue\n","['today', 'city', 'workers', 'state', 'afghan', 'month', 'group', 'melbourne', 'security', 'world', 'union', 'action', 'australia', 'meeting', 'time', 'statement', 'talks', 'man', 'fire', 'reports']\n"]}]},{"cell_type":"code","source":["def predictions_for_nouns(word):\n","  tuple_sim = model.wv.most_similar([word],  topn=100)\n","  synt_predictions = []\n","  print('Predictions for', word)\n","  for i in tuple_sim:\n","    try:\n","      dict_word_tag = dict(wordtags[i[0]])\n","      tag = list(dict_word_tag.keys())[0]\n","      if tag == \"ADJ\":\n","        synt_predictions.append(i[0])\n","    except IndexError:\n","      pass\n","  print(synt_predictions)"],"metadata":{"id":"vVBSqWg8GD5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_for_nouns('food')\n","predictions_for_nouns('time')\n","predictions_for_nouns('world')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLJ9T9qgGGVk","executionInfo":{"status":"ok","timestamp":1666650309828,"user_tz":240,"elapsed":250,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"d169eba1-ee43-49f3-bb4c-def06187bf9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions for food\n","['west', 'first', 'provisional', 'final', 'north', 'other', 'least', 'jewish']\n","Predictions for time\n","['last', 'first', 'australian', 'other', 'international', 'next', 'local', 'general', 'military']\n","Predictions for world\n","['international', 'australian', 'first', 'other', 'local', 'national', 'last', 'military']\n"]}]},{"cell_type":"markdown","source":["**УБРАТЬ ХВОСТ**"],"metadata":{"id":"nbH11a1QGeeq"}},{"cell_type":"code","source":["def remove_tail(word):\n","  tuple_sim = model.wv.most_similar([word])\n","  predictions = []\n","  print('Predictions for', word)\n","  for i in tuple_sim:\n","    cos = i[1]\n","    if cos >= 0.8:\n","      print(i)"],"metadata":{"id":"DST0Y2uWGZz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_tail('food')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwGR2y0VHLJj","executionInfo":{"status":"ok","timestamp":1666651031993,"user_tz":240,"elapsed":3,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"3adc1f7e-62d8-4f56-f682-7aa9baed459e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions for food\n","('here', 0.9871426820755005)\n","('children', 0.9870603680610657)\n","('come', 0.9870145916938782)\n","('reported', 0.9869852066040039)\n","('way', 0.9869130849838257)\n","('already', 0.9868757724761963)\n","('in', 0.9868706464767456)\n","('concerned', 0.9868549704551697)\n","('west', 0.9868288040161133)\n","('storm', 0.9868143796920776)\n"]}]},{"cell_type":"markdown","metadata":{"id":"8my-JVONO-Un"},"source":["Retrieving the vocabulary works the same way:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wqwrB2sWO-Un","executionInfo":{"status":"ok","timestamp":1664881734712,"user_tz":240,"elapsed":342,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"7801f9fd-b7f2-4b7e-b59b-511d51413716"},"outputs":[{"output_type":"stream","name":"stdout","text":["word #0/3000000 is </s>\n","word #1/3000000 is in\n","word #2/3000000 is for\n","word #3/3000000 is that\n","word #4/3000000 is is\n","word #5/3000000 is on\n","word #6/3000000 is ##\n","word #7/3000000 is The\n","word #8/3000000 is with\n","word #9/3000000 is said\n"]}],"source":["for index, word in enumerate(wv.index_to_key):\n","    if index == 10:\n","        break\n","    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"]},{"cell_type":"markdown","metadata":{"id":"Bh0jOHJUO-Uo"},"source":["Storing and loading models\n","--------------------------\n","\n","You'll notice that training non-trivial models can take time.  Once you've\n","trained your model and it works as expected, you can save it to disk.  That\n","way, you don't have to spend time training it all over again later.\n","\n","You can store/load models using the standard gensim methods:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUFrbL2wO-Up"},"outputs":[],"source":["import tempfile\n","\n","with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n","    temporary_filepath = tmp.name\n","    model.save(temporary_filepath)\n","    #\n","    # The model is now safely stored in the filepath.\n","    # You can copy it to other machines, share it with others, etc.\n","    #\n","    # To load a saved model:\n","    #\n","    new_model = gensim.models.Word2Vec.load(temporary_filepath)"]},{"cell_type":"markdown","metadata":{"id":"kASuZL1XO-Up"},"source":["which uses pickle internally, optionally ``mmap``\\ ‘ing the model’s internal\n","large NumPy matrices into virtual memory directly from disk files, for\n","inter-process memory sharing.\n","\n","In addition, you can load models created by the original C tool, both using\n","its text and binary formats::\n","\n","  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n","  # using gzipped/bz2 input works too, no need to unzip\n","  model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B86vwJTpO-Up"},"source":["Training Parameters\n","-------------------\n","\n","``Word2Vec`` accepts several parameters that affect both training speed and quality.\n","\n","min_count\n","---------\n","\n","``min_count`` is for pruning the internal dictionary. Words that appear only\n","once or twice in a billion-word corpus are probably uninteresting typos and\n","garbage. In addition, there’s not enough data to make any meaningful training\n","on those words, so it’s best to ignore them:\n","\n","default value of min_count=5\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prY_fXMnO-Uq"},"outputs":[],"source":["model = gensim.models.Word2Vec(sentences, min_count=10)"]},{"cell_type":"markdown","metadata":{"id":"RGyUjEVXO-Uq"},"source":["vector_size\n","-----------\n","\n","``vector_size`` is the number of dimensions (N) of the N-dimensional space that\n","gensim Word2Vec maps the words onto.\n","\n","Bigger size values require more training data, but can lead to better (more\n","accurate) models. Reasonable values are in the tens to hundreds.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NlNkf9IO-Uq"},"outputs":[],"source":["# The default value of vector_size is 100.\n","model = gensim.models.Word2Vec(sentences, vector_size=200)"]},{"cell_type":"markdown","metadata":{"id":"IRwFIm8ZO-Ur"},"source":["workers\n","-------\n","\n","``workers`` , the last of the major parameters (full list `here\n","<http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>`_)\n","is for training parallelization, to speed up training:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muBxpgV5O-Ur"},"outputs":[],"source":["# default value of workers=3 (tutorial says 1...)\n","model = gensim.models.Word2Vec(sentences, workers=4)"]},{"cell_type":"markdown","metadata":{"id":"UBmPIUhxO-Ur"},"source":["The ``workers`` parameter only has an effect if you have `Cython\n","<http://cython.org/>`_ installed. Without Cython, you’ll only be able to use\n","one core because of the `GIL\n","<https://wiki.python.org/moin/GlobalInterpreterLock>`_ (and ``word2vec``\n","training will be `miserably slow\n","<http://rare-technologies.com/word2vec-in-python-part-two-optimizing/>`_\\ ).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FG4cxvlOO-Us"},"source":["Memory\n","------\n","\n","At its core, ``word2vec`` model parameters are stored as matrices (NumPy\n","arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)\n","times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).\n","\n","Three such matrices are held in RAM (work is underway to reduce that number\n","to two, or even one). So if your input contains 100,000 unique words, and you\n","asked for layer ``vector_size=200``\\ , the model will require approx.\n","``100,000*200*4*3 bytes = ~229MB``.\n","\n","There’s a little extra memory needed for storing the vocabulary tree (100,000 words would\n","take a few megabytes), but unless your words are extremely loooong strings, memory\n","footprint will be dominated by the three matrices above.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mfe3j5McO-Us"},"source":["Evaluating\n","----------\n","\n","``Word2Vec`` training is an unsupervised task, there’s no good way to\n","objectively evaluate the result. Evaluation depends on your end application.\n","\n","Google has released their testing set of about 20,000 syntactic and semantic\n","test examples, following the “A is to B as C is to D” task. It is provided in\n","the 'datasets' folder.\n","\n","For example a syntactic analogy of comparative type is ``bad:worse;good:?``.\n","There are total of 9 types of syntactic comparisons in the dataset like\n","plural nouns and nouns of opposite meaning.\n","\n","The semantic questions contain five types of semantic analogies, such as\n","capital cities (``Paris:France;Tokyo:?``) or family members\n","(``brother:sister;dad:?``).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6PzpvICmO-Us"},"source":["Gensim supports the same evaluation set, in exactly the same format:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKMo9ZN_O-Us","executionInfo":{"status":"ok","timestamp":1664881759853,"user_tz":240,"elapsed":334,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"b1950717-96bb-4d5d-9e67-99dc5985ebc8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.0,\n"," [{'section': 'capital-common-countries',\n","   'correct': [],\n","   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n","    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n","    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n","    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n","    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n","    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]},\n","  {'section': 'capital-world',\n","   'correct': [],\n","   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n","    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]},\n","  {'section': 'currency', 'correct': [], 'incorrect': []},\n","  {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n","  {'section': 'family',\n","   'correct': [],\n","   'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n","    ('HE', 'SHE', 'MAN', 'WOMAN'),\n","    ('HIS', 'HER', 'MAN', 'WOMAN'),\n","    ('HIS', 'HER', 'HE', 'SHE'),\n","    ('MAN', 'WOMAN', 'HE', 'SHE'),\n","    ('MAN', 'WOMAN', 'HIS', 'HER')]},\n","  {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n","  {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n","  {'section': 'gram3-comparative',\n","   'correct': [],\n","   'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n","    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n","    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n","    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n","    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n","    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n","    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n","    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n","    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n","    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n","    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n","    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n","    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n","    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n","    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n","    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n","    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n","    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n","    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n","    ('SMALL', 'SMALLER', 'LOW', 'LOWER')]},\n","  {'section': 'gram4-superlative',\n","   'correct': [],\n","   'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n","    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n","    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n","    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n","    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n","    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n","    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n","    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n","    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n","    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n","    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n","    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]},\n","  {'section': 'gram5-present-participle',\n","   'correct': [],\n","   'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n","    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n","    ('GO', 'GOING', 'RUN', 'RUNNING'),\n","    ('GO', 'GOING', 'SAY', 'SAYING'),\n","    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n","    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n","    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n","    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n","    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n","    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n","    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n","    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n","    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n","    ('RUN', 'RUNNING', 'GO', 'GOING'),\n","    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n","    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n","    ('SAY', 'SAYING', 'GO', 'GOING'),\n","    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n","    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n","    ('SAY', 'SAYING', 'RUN', 'RUNNING')]},\n","  {'section': 'gram6-nationality-adjective',\n","   'correct': [],\n","   'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n","    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n","    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n","    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n","    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n","    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n","    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n","    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n","    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n","    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n","    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n","    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n","    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n","    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n","    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n","    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n","    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n","    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n","    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n","    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n","    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]},\n","  {'section': 'gram7-past-tense',\n","   'correct': [],\n","   'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n","    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n","    ('GOING', 'WENT', 'SAYING', 'SAID'),\n","    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n","    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n","    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n","    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n","    ('PAYING', 'PAID', 'GOING', 'WENT'),\n","    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n","    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n","    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n","    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n","    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n","    ('SAYING', 'SAID', 'GOING', 'WENT'),\n","    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n","    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n","    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n","    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n","    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n","    ('TAKING', 'TOOK', 'SAYING', 'SAID')]},\n","  {'section': 'gram8-plural',\n","   'correct': [],\n","   'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n","    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n","    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n","    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n","    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n","    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n","    ('CAR', 'CARS', 'MAN', 'MEN'),\n","    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n","    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n","    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n","    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n","    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n","    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n","    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n","    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n","    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n","    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n","    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n","    ('MAN', 'MEN', 'CAR', 'CARS'),\n","    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n","    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n","    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n","    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n","    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n","    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n","    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n","    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n","    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n","    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n","    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]},\n","  {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []},\n","  {'section': 'Total accuracy',\n","   'correct': [],\n","   'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n","    ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n","    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n","    ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n","    ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n","    ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'),\n","    ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n","    ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n","    ('HE', 'SHE', 'HIS', 'HER'),\n","    ('HE', 'SHE', 'MAN', 'WOMAN'),\n","    ('HIS', 'HER', 'MAN', 'WOMAN'),\n","    ('HIS', 'HER', 'HE', 'SHE'),\n","    ('MAN', 'WOMAN', 'HE', 'SHE'),\n","    ('MAN', 'WOMAN', 'HIS', 'HER'),\n","    ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n","    ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n","    ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n","    ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n","    ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n","    ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n","    ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n","    ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n","    ('LONG', 'LONGER', 'LOW', 'LOWER'),\n","    ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n","    ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n","    ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n","    ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n","    ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n","    ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n","    ('LOW', 'LOWER', 'LONG', 'LONGER'),\n","    ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n","    ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n","    ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n","    ('SMALL', 'SMALLER', 'LOW', 'LOWER'),\n","    ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n","    ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n","    ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n","    ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n","    ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n","    ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n","    ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n","    ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n","    ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n","    ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n","    ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n","    ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n","    ('GO', 'GOING', 'LOOK', 'LOOKING'),\n","    ('GO', 'GOING', 'PLAY', 'PLAYING'),\n","    ('GO', 'GOING', 'RUN', 'RUNNING'),\n","    ('GO', 'GOING', 'SAY', 'SAYING'),\n","    ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n","    ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n","    ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n","    ('LOOK', 'LOOKING', 'GO', 'GOING'),\n","    ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n","    ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n","    ('PLAY', 'PLAYING', 'GO', 'GOING'),\n","    ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n","    ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n","    ('RUN', 'RUNNING', 'GO', 'GOING'),\n","    ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n","    ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n","    ('SAY', 'SAYING', 'GO', 'GOING'),\n","    ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n","    ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n","    ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n","    ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n","    ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n","    ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n","    ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n","    ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n","    ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n","    ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n","    ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n","    ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n","    ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n","    ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n","    ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n","    ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n","    ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n","    ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n","    ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n","    ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n","    ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n","    ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n","    ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n","    ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n","    ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'),\n","    ('GOING', 'WENT', 'PAYING', 'PAID'),\n","    ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n","    ('GOING', 'WENT', 'SAYING', 'SAID'),\n","    ('GOING', 'WENT', 'TAKING', 'TOOK'),\n","    ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n","    ('PAYING', 'PAID', 'SAYING', 'SAID'),\n","    ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n","    ('PAYING', 'PAID', 'GOING', 'WENT'),\n","    ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n","    ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n","    ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n","    ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n","    ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n","    ('SAYING', 'SAID', 'GOING', 'WENT'),\n","    ('SAYING', 'SAID', 'PAYING', 'PAID'),\n","    ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n","    ('TAKING', 'TOOK', 'GOING', 'WENT'),\n","    ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n","    ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n","    ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n","    ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n","    ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n","    ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n","    ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n","    ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n","    ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n","    ('CAR', 'CARS', 'MAN', 'MEN'),\n","    ('CAR', 'CARS', 'ROAD', 'ROADS'),\n","    ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n","    ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n","    ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n","    ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n","    ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n","    ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n","    ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n","    ('MAN', 'MEN', 'ROAD', 'ROADS'),\n","    ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n","    ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n","    ('MAN', 'MEN', 'CAR', 'CARS'),\n","    ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n","    ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n","    ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n","    ('ROAD', 'ROADS', 'CAR', 'CARS'),\n","    ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n","    ('ROAD', 'ROADS', 'MAN', 'MEN'),\n","    ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n","    ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n","    ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n","    ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n","    ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}])"]},"metadata":{},"execution_count":24}],"source":["model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"]},{"cell_type":"markdown","metadata":{"id":"Tb80MYmgO-Ut"},"source":["This ``evaluate_word_analogies`` method takes an `optional parameter\n","<http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>`_\n","``restrict_vocab`` which limits which test examples are to be considered.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sNZK5wS2O-Ut"},"source":["In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n","\n","By default it uses an academic dataset WS-353 but one can create a dataset\n","specific to your business based on it. It contains word pairs together with\n","human-assigned similarity judgments. It measures the relatedness or\n","co-occurrence of two words. For example, 'coast' and 'shore' are very similar\n","as they appear in the same context. At the same time 'clothes' and 'closet'\n","are less similar because they are related but not interchangeable.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T14EQnA6O-Ut","executionInfo":{"status":"ok","timestamp":1664881763045,"user_tz":240,"elapsed":347,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"2afd43bf-0efe-4622-f819-0a6baae4c759"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((0.15785039102452428, 0.22837627046937706),\n"," SpearmanrResult(correlation=0.11554814577429677, pvalue=0.3793177755196202),\n"," 83.0028328611898)"]},"metadata":{},"execution_count":25}],"source":["model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"]},{"cell_type":"markdown","metadata":{"id":"DoWXWKISO-Uu"},"source":[".. Important::\n","  Good performance on Google's or WS-353 test set doesn’t mean word2vec will\n","  work well in your application, or vice versa. It’s always best to evaluate\n","  directly on your intended task. For an example of how to use word2vec in a\n","  classifier pipeline, see this `tutorial\n","  <https://github.com/RaRe-Technologies/movie-plots-by-genre>`_.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sy7L35MqO-Uu"},"source":["Online training / Resuming training\n","-----------------------------------\n","\n","Advanced users can load a model and continue training it with more sentences\n","and `new vocabulary words <online_w2v_tutorial.ipynb>`_:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tk1QUO8rO-Uu","executionInfo":{"status":"ok","timestamp":1664881765933,"user_tz":240,"elapsed":329,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"77a5155a-2c15-4cd1-bb37-c9ea8974d291"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"]}],"source":["model = gensim.models.Word2Vec.load(temporary_filepath)\n","more_sentences = [\n","    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n","     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n","]\n","model.build_vocab(more_sentences, update=True)\n","model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n","\n","# cleaning up temporary file\n","import os\n","os.remove(temporary_filepath)"]},{"cell_type":"markdown","metadata":{"id":"b4RwugfMO-Uu"},"source":["You may need to tweak the ``total_words`` parameter to ``train()``,\n","depending on what learning rate decay you want to simulate.\n","\n","Note that it’s not possible to resume training with models generated by the C\n","tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\n","querying/similarity, but information vital for training (the vocab tree) is\n","missing there.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yky_LDk3O-Uv"},"source":["Training Loss Computation\n","-------------------------\n","\n","The parameter ``compute_loss`` can be used to toggle computation of loss\n","while training the Word2Vec model. The computed loss is stored in the model\n","attribute ``running_training_loss`` and can be retrieved using the function\n","``get_latest_training_loss`` as follows :\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OA0cP_nO-Uv","executionInfo":{"status":"ok","timestamp":1664881769966,"user_tz":240,"elapsed":1854,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"7bf7b1f4-64f4-4c13-a9c2-a747731c0eca"},"outputs":[{"output_type":"stream","name":"stdout","text":["1245371.375\n"]}],"source":["# instantiating and training the Word2Vec model\n","model_with_loss = gensim.models.Word2Vec(\n","    sentences,\n","    min_count=1,\n","    compute_loss=True,\n","    hs=0,\n","    sg=1,\n","    seed=42,\n","    window = 3\n",")\n","\n","# getting the training loss value\n","training_loss = model_with_loss.get_latest_training_loss()\n","print(training_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwLdzi3oUrnv","executionInfo":{"status":"ok","timestamp":1664881774566,"user_tz":240,"elapsed":1788,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"b618490d-4aa1-46b2-c161-67d074132681"},"outputs":[{"output_type":"stream","name":"stdout","text":["1499113.5\n"]}],"source":["# instantiating and training the Word2Vec model\n","model_with_loss = gensim.models.Word2Vec(\n","    sentences,\n","    min_count=1,\n","    compute_loss=True,\n","    hs=0,\n","    sg=1,\n","    seed=42,\n","    window = 5\n",")\n","\n","# getting the training loss value\n","training_loss = model_with_loss.get_latest_training_loss()\n","print(training_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVyWNkYNUt5-","executionInfo":{"status":"ok","timestamp":1664881779011,"user_tz":240,"elapsed":2227,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"565cb692-70e0-4bd3-b9bb-2c9ec4d80552"},"outputs":[{"output_type":"stream","name":"stdout","text":["2088718.875\n"]}],"source":["# instantiating and training the Word2Vec model\n","model_with_loss = gensim.models.Word2Vec(\n","    sentences,\n","    min_count=1,\n","    compute_loss=True,\n","    hs=0,\n","    sg=1,\n","    seed=42,\n","    window = 8\n",")\n","\n","# getting the training loss value\n","training_loss = model_with_loss.get_latest_training_loss()\n","print(training_loss)"]},{"cell_type":"markdown","metadata":{"id":"QlhibytzO-Uv"},"source":["Benchmarks\n","----------\n","\n","Let's run some benchmarks to see effect of the training loss computation code\n","on training time.\n","\n","We'll use the following data for the benchmarks:\n","\n","#. Lee Background corpus: included in gensim's test data\n","#. Text8 corpus.  To demonstrate the effect of corpus size, we'll look at the\n","   first 1MB, 10MB, 50MB of the corpus, as well as the entire thing.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxObS6M_O-Uw","executionInfo":{"status":"ok","timestamp":1664881813973,"user_tz":240,"elapsed":31601,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7a86fd3-c3ce-4347-eb97-e47b33c640a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 31.6/31.6MB downloaded\n"]}],"source":["import io\n","import os\n","\n","import gensim.models.word2vec\n","import gensim.downloader as api\n","import smart_open\n","\n","\n","def head(path, size):\n","    with smart_open.open(path) as fin:\n","        return io.StringIO(fin.read(size))\n","\n","\n","def generate_input_data():\n","    lee_path = datapath('lee_background.cor')\n","    ls = gensim.models.word2vec.LineSentence(lee_path)\n","    ls.name = '25kB'\n","    yield ls\n","\n","    text8_path = api.load('text8').fn\n","    labels = ('1MB', '10MB', '50MB', '100MB')\n","    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n","    for l, s in zip(labels, sizes):\n","        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n","        ls.name = l\n","        yield ls\n","\n","\n","input_data = list(generate_input_data())"]},{"cell_type":"markdown","metadata":{"id":"bPVegEqRO-Uw"},"source":["We now compare the training time taken for different combinations of input\n","data and model training parameters like ``hs`` and ``sg``.\n","\n","For each combination, we repeat the test several times to obtain the mean and\n","standard deviation of the test duration.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJf8MPoQO-Ux","executionInfo":{"status":"ok","timestamp":1664882574412,"user_tz":240,"elapsed":744686,"user":{"displayName":"Tanya Novosiltseva","userId":"12988809638898186261"}},"outputId":"4a5198ef-b5b8-47c2-bc83-0af1b231ade3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word2vec model #0: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.2731333573659261, 'train_time_std': 0.007982863492415703}\n","Word2vec model #1: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.2780876159667969, 'train_time_std': 0.01069209329465796}\n","Word2vec model #2: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.486361821492513, 'train_time_std': 0.004089898886394452}\n","Word2vec model #3: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.4923876126607259, 'train_time_std': 0.009908616564309117}\n","Word2vec model #4: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 0.6358914375305176, 'train_time_std': 0.006314706041840174}\n","Word2vec model #5: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 0.6231373945871989, 'train_time_std': 0.00870742496793421}\n","Word2vec model #6: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 1.2368848323822021, 'train_time_std': 0.014744150624246563}\n","Word2vec model #7: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 1.2001901467641194, 'train_time_std': 0.0026801859490536333}\n","Word2vec model #8: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.8287514845530192, 'train_time_std': 0.0006652366949904754}\n","Word2vec model #9: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.8431979020436605, 'train_time_std': 0.010901034925987806}\n","Word2vec model #10: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 1.597856839497884, 'train_time_std': 0.0159333317105362}\n","Word2vec model #11: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 1.6158099174499512, 'train_time_std': 0.06609599980135814}\n","Word2vec model #12: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 2.177928845087687, 'train_time_std': 0.027655580166597134}\n","Word2vec model #13: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 2.1806461016337075, 'train_time_std': 0.009609900380839188}\n","Word2vec model #14: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 4.4197328090667725, 'train_time_std': 0.0237870773148966}\n","Word2vec model #15: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 4.341651439666748, 'train_time_std': 0.039423015508790855}\n","Word2vec model #16: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 9.687307596206665, 'train_time_std': 0.1559003580763898}\n","Word2vec model #17: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 9.62166945139567, 'train_time_std': 0.08572250838215174}\n","Word2vec model #18: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 18.656464258829754, 'train_time_std': 0.42756368022720814}\n","Word2vec model #19: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 18.75311342875163, 'train_time_std': 0.9565154642666258}\n","Word2vec model #20: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 28.011224428812664, 'train_time_std': 0.5708201364666731}\n","Word2vec model #21: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 27.410297711690266, 'train_time_std': 0.7049473266705666}\n","Word2vec model #22: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 58.06173570950826, 'train_time_std': 0.7318240413483769}\n","Word2vec model #23: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 54.475541830062866, 'train_time_std': 1.0739178899726687}\n","   train_data  compute_loss  sg  hs  train_time_mean  train_time_std\n","4        25kB          True   1   0         0.635891        0.006315\n","5        25kB         False   1   0         0.623137        0.008707\n","6        25kB          True   1   1         1.236885        0.014744\n","7        25kB         False   1   1         1.200190        0.002680\n","0        25kB          True   0   0         0.273133        0.007983\n","1        25kB         False   0   0         0.278088        0.010692\n","2        25kB          True   0   1         0.486362        0.004090\n","3        25kB         False   0   1         0.492388        0.009909\n","12        1MB          True   1   0         2.177929        0.027656\n","13        1MB         False   1   0         2.180646        0.009610\n","14        1MB          True   1   1         4.419733        0.023787\n","15        1MB         False   1   1         4.341651        0.039423\n","8         1MB          True   0   0         0.828751        0.000665\n","9         1MB         False   0   0         0.843198        0.010901\n","10        1MB          True   0   1         1.597857        0.015933\n","11        1MB         False   0   1         1.615810        0.066096\n","20       10MB          True   1   0        28.011224        0.570820\n","21       10MB         False   1   0        27.410298        0.704947\n","22       10MB          True   1   1        58.061736        0.731824\n","23       10MB         False   1   1        54.475542        1.073918\n","16       10MB          True   0   0         9.687308        0.155900\n","17       10MB         False   0   0         9.621669        0.085723\n","18       10MB          True   0   1        18.656464        0.427564\n","19       10MB         False   0   1        18.753113        0.956515\n"]}],"source":["# Temporarily reduce logging verbosity\n","logging.root.level = logging.ERROR\n","\n","import time\n","import numpy as np\n","import pandas as pd\n","\n","train_time_values = []\n","seed_val = 42\n","sg_values = [0, 1]\n","hs_values = [0, 1]\n","\n","fast = True\n","if fast:\n","    input_data_subset = input_data[:3]\n","else:\n","    input_data_subset = input_data\n","\n","\n","for data in input_data_subset:\n","    for sg_val in sg_values:\n","        for hs_val in hs_values:\n","            for loss_flag in [True, False]:\n","                time_taken_list = []\n","                for i in range(3):\n","                    start_time = time.time()\n","                    w2v_model = gensim.models.Word2Vec(\n","                        data,\n","                        compute_loss=loss_flag,\n","                        sg=sg_val,\n","                        hs=hs_val,\n","                        seed=seed_val,\n","                    )\n","                    time_taken_list.append(time.time() - start_time)\n","\n","                time_taken_list = np.array(time_taken_list)\n","                time_mean = np.mean(time_taken_list)\n","                time_std = np.std(time_taken_list)\n","\n","                model_result = {\n","                    'train_data': data.name,\n","                    'compute_loss': loss_flag,\n","                    'sg': sg_val,\n","                    'hs': hs_val,\n","                    'train_time_mean': time_mean,\n","                    'train_time_std': time_std,\n","                }\n","                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n","                train_time_values.append(model_result)\n","\n","train_times_table = pd.DataFrame(train_time_values)\n","train_times_table = train_times_table.sort_values(\n","    by=['train_data', 'sg', 'hs', 'compute_loss'],\n","    ascending=[False, False, True, False],\n",")\n","print(train_times_table)"]},{"cell_type":"markdown","metadata":{"id":"Ko78u1CMO-Ux"},"source":["Visualising Word Embeddings\n","---------------------------\n","\n","The word embeddings made by the model can be visualised by reducing\n","dimensionality of the words to 2 dimensions using tSNE.\n","\n","Visualisations can be used to notice semantic and syntactic trends in the data.\n","\n","Example:\n","\n","* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n","* Syntactic: words like run, running or cut, cutting lie close together.\n","\n","Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n","\n",".. Important::\n","  The model used for the visualisation is trained on a small corpus. Thus\n","  some of the relations might not be so clear.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZcRj1avO-Uy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7d51a35-5c1d-4fb9-f2b5-88585e8ad9a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n","  FutureWarning,\n"]}],"source":["from sklearn.decomposition import IncrementalPCA    # inital reduction\n","from sklearn.manifold import TSNE                   # final reduction\n","import numpy as np                                  # array handling\n","\n","\n","def reduce_dimensions(model):\n","    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n","\n","    # extract the words & their vectors, as numpy arrays\n","    vectors = np.asarray(model.wv.vectors)\n","    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n","\n","    # reduce using t-SNE\n","    tsne = TSNE(n_components=num_dimensions, random_state=0)\n","    vectors = tsne.fit_transform(vectors)\n","\n","    x_vals = [v[0] for v in vectors]\n","    y_vals = [v[1] for v in vectors]\n","    return x_vals, y_vals, labels\n","\n","\n","x_vals, y_vals, labels = reduce_dimensions(model)\n","\n","def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n","    from plotly.offline import init_notebook_mode, iplot, plot\n","    import plotly.graph_objs as go\n","\n","    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n","    data = [trace]\n","\n","    if plot_in_notebook:\n","        init_notebook_mode(connected=True)\n","        iplot(data, filename='word-embedding-plot')\n","    else:\n","        plot(data, filename='word-embedding-plot.html')\n","\n","\n","def plot_with_matplotlib(x_vals, y_vals, labels):\n","    import matplotlib.pyplot as plt\n","    import random\n","\n","    random.seed(0)\n","\n","    plt.figure(figsize=(12, 12))\n","    plt.scatter(x_vals, y_vals)\n","\n","    #\n","    # Label randomly subsampled 25 data points\n","    #\n","    indices = list(range(len(labels)))\n","    selected_indices = random.sample(indices, 25)\n","    for i in selected_indices:\n","        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n","\n","try:\n","    get_ipython()\n","except Exception:\n","    plot_function = plot_with_matplotlib\n","else:\n","    plot_function = plot_with_plotly\n","\n","plot_function(x_vals, y_vals, labels)"]},{"cell_type":"markdown","metadata":{"id":"LCz-KKQkO-Uy"},"source":["Conclusion\n","----------\n","\n","In this tutorial we learned how to train word2vec models on your custom data\n","and also how to evaluate it. Hope that you too will find this popular tool\n","useful in your Machine Learning tasks!\n","\n","Links\n","-----\n","\n","- API docs: :py:mod:`gensim.models.word2vec`\n","- `Original C toolkit and word2vec papers by Google <https://code.google.com/archive/p/word2vec/>`_.\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["Bh0jOHJUO-Uo","kASuZL1XO-Up","B86vwJTpO-Up","FG4cxvlOO-Us","QlhibytzO-Uv","Ko78u1CMO-Ux","LCz-KKQkO-Uy"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}